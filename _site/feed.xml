<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-05-09T11:54:51+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Science to Technology</title><subtitle>Welcome to &quot;S2T.&quot; This is a blog that provides some of the things I've learned while looking into the tech sector. I hope you can find tools and resources that may help you in this exciting field!</subtitle><entry><title type="html">The Yield Curve: Current Trends</title><link href="http://localhost:4000/2019/05/09/yield-curve-project.html" rel="alternate" type="text/html" title="The Yield Curve: Current Trends" /><published>2019-05-09T00:00:00+01:00</published><updated>2019-05-09T00:00:00+01:00</updated><id>http://localhost:4000/2019/05/09/yield-curve-project</id><content type="html" xml:base="http://localhost:4000/2019/05/09/yield-curve-project.html">&lt;p&gt;&lt;img src=&quot;/assets/img/yield_curve_current_Mar19.png&quot; width=&quot;700&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;understanding-the-yield-curve-an-economic-predictor&quot;&gt;Understanding The Yield Curve: An Economic Predictor&lt;/h2&gt;
&lt;p&gt;In this project, we’ll aim to understand what the yield curve is, how it can be used as a predictive tool, and gain knowledge on what the current U.S. Treasury yield curve might be telling us about the economy.&lt;/p&gt;

&lt;p&gt;We’ll analyze historical data on the daily Treasury yield curve rates. To analyze what the current yield curve might be telling us, we’ll aim to determine:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The U.S. Treasury current yield curve shape&lt;/li&gt;
  &lt;li&gt;Comparitive historical yield curve shapes&lt;/li&gt;
  &lt;li&gt;The spread between short- and long-term securities&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-is-the-yield-curve-and-why-is-it-important&quot;&gt;What is the yield curve and why is it important?&lt;/h3&gt;

&lt;p&gt;A yield curve is a graphical representation that shows the relationship between the yield (i.e., interest rate, which is the money you would get back if you lent your money to the government) on securities and the maturity (i.e., duration, or the time you loan your money to the government).&lt;/p&gt;

&lt;p&gt;Historically, the yield curve has reflected the market’s sense of the economy. The yield curve is important as it can be used to predict future market conditions.&lt;/p&gt;

&lt;h3 id=&quot;summary-of-results&quot;&gt;Summary of Results&lt;/h3&gt;

&lt;p&gt;After analyzing the &lt;a href=&quot;https://www.treasury.gov/resource-center/data-chart-center/interest-rates/pages/TextView.aspx?data=yieldYear&amp;amp;year=2019&quot;&gt;data&lt;/a&gt;, the conclusion reached is that the current U.S. treasury yield curve appears to be telling us that there might be a gradual slowdown in economic growth in 2019. And that it is possible that a recession could happen some time in 2020 to 2021. Please note that this is only indicative of an economic slowdown and upcoming recession. It is not definitive, within the time periods stated.&lt;/p&gt;

&lt;p&gt;For the full details of the project, please refer to the analysis found &lt;a href=&quot;https://github.com/sunshinescience/yield_curve/blob/master/yieldcurve.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Dimensionality Reduction and Clustering on the Handwritten Digits Dataset</title><link href="http://localhost:4000/2019/04/27/dimensionality-reduction-clustering.html" rel="alternate" type="text/html" title="Dimensionality Reduction and Clustering on the Handwritten Digits Dataset" /><published>2019-04-27T00:00:00+01:00</published><updated>2019-04-27T00:00:00+01:00</updated><id>http://localhost:4000/2019/04/27/dimensionality-reduction-clustering</id><content type="html" xml:base="http://localhost:4000/2019/04/27/dimensionality-reduction-clustering.html">&lt;p&gt;&lt;img src=&quot;/assets/img/gear_head_concept.jpg&quot; width=&quot;500&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of the goals of science is the pursuit of knowledge. And one of the goals of technology is to create products that solve problems. As technology appears to be the practical application of science, I think that applying machine learning (ML) techniques is exciting! ML is a ubiquitous technology –it is found everywhere! Just a few applications include the type of product a person might buy, the ability for cars to drive themselves, determining if an email is spam and moving it into your spam folder, and the list goes on…&lt;/p&gt;

&lt;p&gt;My interest in machine learning piqued as I learned what an incredible tool it can be for transforming information into knowledge. These techniques provide various ways to solve a problem.  Machine learning is a method of providing systems the ability to learn and improve from experience in an automated fashion, without the need for explicitly being programmed.  Two of the broad categories in which machine learning tasks are classified into are: supervised learning and unsupervised learning.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Unsupervised learning&lt;/strong&gt; essentially learns from unlabelled and uncategorized input data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Supervised learning&lt;/strong&gt; undertakes the task of learning a mapping function based on input-output pairs. This task learns the mapping function so well that it maps an input to predict an output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have found scikit-learn incredibly useful. Scikit-learn supports several categories of algorithms, such as regression, classification, clustering, and dimensionality reduction. It offers tools that allow you to find the best model for your data (model selection). The scikit-learn algorithm cheat-sheet can be found &lt;a href=&quot;https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html&quot;&gt;here&lt;/a&gt;. Moreover, scikit-learn has tools for data processing and preparation (&lt;a href=&quot;https://scikit-learn.org/stable/modules/preprocessing.html&quot;&gt;preprocessing&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;As a scientist, I really enjoyed finding patterns and structure in various forms of data. An interesting machine learning project to delve into may be clustering, as one of the goals of clustering is to identify structure in data. And dimensionality reduction might improve the clustering of the data. Thus, in this demonstration, dimensionality reduction and subsequent clustering methods will be explored. While I would like to apply this to high dimensional unlabelled data, I would initially like to work with a labelled dataset. First, I’ll assess the handwritten digits dataset, as it has been tagged with labels. I’ll use Python for this demonstration.&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;For this demonstration, the handwritten digits dataset from scikit-learn will be used. Scikit-learn has a copy of the test set of the UCI ML hand-written digits &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;dataset&lt;/a&gt;. An example to load and visualize the data via scikit-learn can be found &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html&quot;&gt;here&lt;/a&gt;. The dataset characteristics on scikit-learn can be found in section &lt;a href=&quot;https://scikit-learn.org/stable/datasets/index.html#digits-dataset&quot;&gt;5.2.4&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This dataset is made up of 1797 images of hand-written digits. There are ten classes, in which each class refers to a handwritten digit (0 to 9). See an example of digits 0 to 9 in the images below. The axes are the pixel numbers, for each 8x8 pixel image.
&lt;img src=&quot;/assets/img/digits_10example.png&quot; width=&quot;500&quot; height=&quot;250&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s first import a few Python libraries that are useful. Let’s also import some libraries from scikit-learn and load the dataset.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import load_digits
from sklearn.preprocessing import scale

# Load the handwritten digits dataset
digits = load_digits()

# Standardize a dataset along any axis 
data = scale(digits.data) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code will generate the above figure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Plot of ten example digits (from 0 to 9)
fig = plt.figure(figsize=(10,5))
plt_index = 0
for i in range(10):
    plt_index = plt_index + 1
    ax = fig.add_subplot(2, 5, plt_index)
    ax.imshow(digits.images[i], cmap=plt.cm.gray_r, interpolation='nearest')
plt.tight_layout()
plt.show() 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each individual label indicates which digit (0 to 9) is captured in that particular image. When plotting, each individual data point represents an 8x8 image of a digit. Each 8x8 image has been pre-processed such that the digit fits and is centered. Generally if an image is in grayscale, the tuple returned contains only the number of rows and columns, whereas a color image would have a tuple returned of rows, columns, and channels. In this example, these images are grayscaled as the tuple returned is 8x8 using the code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print (digits.images[0].shape)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Generally, the dimensionality in an image is the number of pixels within its image. In this case, each image consists of 64 pixels that represent the features of that particular digit. A feature can be thought of as an individual measureable property of something that is being observed. Thus, instead of thinking of each image as an 8x8 grid of pixels, we can consider it as a vector, or list, of 64 features.&lt;/p&gt;

&lt;p&gt;A figure of four of each of the nine digits is shown in order to illustrate differences contributed by participants whom had written the digits. The ticks and corresponding tick labels have been removed for ease of view. See the plot and corresponding code below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Plot of four examples from each of the 10 digits (0 through 9)
sample_lst = [] # A list of lists. Each list within contains four sample numbers of a single digit, and it goes from digit 0 (the first list within the list of lists) to digit 9 (the last list)
for i in range(10):
    sample_nums = np.where(labels == i)[0][0:4] # Indexes (0:4) into an array of sample numbers that are a specified digit (the digit is written as i in the loop)
    sample_nums = sample_nums.tolist() # Convert the array called sample_nums to a list with the same items
    sample_lst.append(sample_nums) 
if 0:
    fig = plt.figure(figsize=(10,6))
    plt_index = 0
    for i in range(0,10):
        for j in sample_lst[i]:
            plt_index = plt_index + 1
            ax = fig.add_subplot(5, 8, plt_index)
            ax.imshow(digits.images[j], cmap=plt.cm.gray_r, interpolation='nearest')
            ax.set_yticklabels([]) # Turn off y tick labels
            ax.set_xticklabels([]) # Turn off x tick labels
            ax.set_yticks([]) # Turn off y ticks
            ax.set_xticks([]) # Turn off x ticks
    plt.tight_layout()
    plt.show()  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/digits_each_4.png&quot; width=&quot;700&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;preprocessing&quot;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;https://scikit-learn.org/stable/modules/preprocessing.html&quot;&gt;sklearn.preprocessing&lt;/a&gt; package contains numerous functions and classes to change features into a representation that would be useful for learning algorithms. It may be good practice to initially standardize a dataset, as it is usually advantageous to apply learning algorithms to a standardized dataset. If the dataset contains outliers, scalars or transformers might be more beneficial.&lt;/p&gt;

&lt;p&gt;The goal in standardizing and normalizing is to change the values (e.g., numeric feature values within a column) in order to use a common scale, without distorting differences in the ranges of values or losing information. This can help the model overcome learning problems.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Data &lt;em&gt;normalization&lt;/em&gt; is the process of scaling individual samples to have unit norm (i.e., a vector of length 1). When a feature is normalized, the feature values are in between 0 and 1.&lt;/li&gt;
  &lt;li&gt;Data &lt;em&gt;standardization&lt;/em&gt; is the process of transforming features so that each feature looks like standard normally distributed data. And each feature is distributed about zero with a standard deviation of 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The learning algorithms might not work as well if the individual features do not appear similar to standard normally distributed data (i.e., Gaussian with zero mean and unit variance). For example, it is assumed by elements in the objective function of a learning algorithm that all features are centered around zero and have similar variances. But, if a feature has a variance that is much larger than other features, it might dominate the objective function of the learning algorithm and thus make the algorithm unable to learn from other features correctly. Here, the data is standardized in the code above.&lt;/p&gt;

&lt;h2 id=&quot;dimensionality-reduction&quot;&gt;Dimensionality reduction&lt;/h2&gt;
&lt;p&gt;Dimensionality is the number of features in your dataset. Dimensionality reduction is a process that reduces that number of features. It might be best to try to attain a compact representation of high-dimensional data without having to lose much information. There are many benefits in its applications. One advantage is that it reduces the processing time and storage space needed for the particular modelling algorithm. It may help avoid the effects of the curse of dimensionality. It can also improve the interpretation of the parameters of the machine learning model. Also, in order to visualize data, it needs to be reduced to very low dimensions (2D or 3D). There appears to be two key dimensionality reduction methods:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Feature selection – A subset is selected from the original feature set.&lt;/li&gt;
  &lt;li&gt;Feature extraction - A new (smaller) set of features is created from the original feature set. This new set of features captures the most useful information in the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A guide that explains both feature selection and feature extraction in detail can be found &lt;a href=&quot;https://elitedatascience.com/dimensionality-reduction-algorithms&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For comparison purposes, the following dimensionality reduction techniques will be used: principal component analysis, t-distributed Stochastic Neighbor Embedding, truncated singular value decomposition, and Isomap.&lt;/p&gt;

&lt;p&gt;Let’s import some libraries from scikit-learn:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from sklearn import metrics
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.decomposition import TruncatedSVD
from sklearn.manifold import Isomap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;principal-component-analysis&quot;&gt;Principal component analysis&lt;/h3&gt;
&lt;p&gt;Principal component analysis (PCA) is a technique that reduces dimensionality of a dataset that comprises several interrelated variables, while retaining much of the variation within the dataset. This occurs by transforming to a new set of variables, the principal components (PCs). These PCs are not correlated and they are ordered such that the first few of them retain most of the variation contained within all of the original variables.&lt;/p&gt;

&lt;p&gt;The idea of PCA is to express the data in terms of a few well-chosen vectors that more efficiently capture the variation in that data. Generally, the goal is to save space while avoiding redundancies and eliminating the least important information. PCA is one of the most widely used techniques for dimensionality reduction and it is often used with large datasets with many features. 
This technique finds a combination of features that capture the variance of the original features well. Thus, I thought it best to use this technique first in order to have a look at what it does to the dataset. I won’t delve into the mathematics here, but the derivation of the principal components (and much additional information) can be found &lt;a href=&quot;https://www.springer.com/gb/book/9780387954424&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, let’s visualize the original data (i.e., this data is not standardized). We’ll project the original data from high-dimensional (64 dimensions) space into lower-dimensional principal component space (2 dimensions). We’ll color the data points by their corresponding digit class, and thus see how well these classes have been separated in principal component space.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# The first two principal components are generated below (on the non-standardized data)
pca2_result = PCA(n_components=2).fit_transform(digits.data) 

def plot_dim_red(dim_red, data, title=None, xlabel=None, ylabel=None):
    &quot;&quot;&quot;
    Scatter plot of dimensionality reduced data.
    dim_red: dimensionality reduced data
    data: data
    title: string of the title
    xlabel: string of the x label
    ylabel: string of the y label
    &quot;&quot;&quot;
    plt.scatter(dim_red[:, 0], dim_red[:, 1], c=digits.target, cmap=plt.cm.get_cmap('tab10', 10), marker='o', edgecolor='none', alpha=0.5) 
    plt.colorbar(ticks=range(10))
    plt.clim(-0.5, 9.5)
    if xlabel:
        plt.xlabel('Component 1')
    if ylabel:
        plt.ylabel('Component 2')
    if title:
        plt.title(title)
    plt.show()

plot_dim_red(pca2_result, data, 'PCA reduced handwritten digits', 'Component 1', 'Component 2')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pca_digits_non_standardized.png&quot; width=&quot;700&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, the explained variation per principal component is ~12% and ~9%. Thus, the first two principal components account for ~22% of the variation in the entire dataset. See the code below:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pca = PCA(n_components=2) # The original 64 dimensions are reduced to 2   
pca_result = pca.fit_transform(data) # Now we have the reduced feature set  
print ('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))  
print ('The first two components account for {:.0f}% of the variation in the entire dataset'.format((pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1])*100))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And ~50% of the variance is contained within 5 principal components. One may estimate how many components are needed to describe the data by assessing the cumulative explained variance ratio as a function of the number of components. Approximately 90% of the variance is retained within 20 components, whereas 75% is retained within 10 components. See the code and corresponding plot below of the cumulative sum of the percentage of variance explained by the components.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pca4 = PCA().fit(digits.data) # Conducting PCA without specifying a number of
plt.plot(np.cumsum(pca4.explained_variance_ratio_)) # The cumulative sum of the percentage of variance explained by all of the components
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
plt.title('Number of components vs. retained variance')
plt.show() 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/n_comp_var.png&quot; width=&quot;650&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This data from scikit-learn is stored in the .data member, which is an (n_samples, n_features) array. This array changed from the initial shape of the array (1797, 64) to the current shape (1797, 2) via PCA reduction. Thus, the vector length has now been compressed to a length of 2. But has important data been lost by conducting this PCA reduction? Let’s plot the inverse transform of the PCA reduced data below the original data. The inverse transform can help illustrate the reconstructed image data after having undergone PCA reduction.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;percnt = 2 
pca3 = PCA(n_components=percnt).fit(digits.data) 
pca3_result = pca3.transform(digits.data)
# Transform the PCA reduced data back to its original space. 
pca_inversed = pca3.inverse_transform(pca3_result)  # Use the inverse of the transform to reconstruct the reduced digits

# Figure of original images, as assigned to the variable called inversed_lst
inversed_lst = range(0, 10)
fig = plt.figure(figsize=(10,2))
plt_index = 0
for i in inversed_lst:
    plt_index = plt_index + 1
    ax = fig.add_subplot(1, 10, plt_index)
    ax.imshow(digits.images[i], cmap=plt.cm.gray_r, interpolation='nearest')
plt.tight_layout()
plt.show() 

# Figure of images that have undergone PCA reduction, as assigned to the variable called inversed_lst. The inverse transform is plotted here
fig = plt.figure(figsize=(10,2))
plt_index = 0
for i in inversed_lst:
    plt_index = plt_index + 1
    ax = fig.add_subplot(1, 10, plt_index)
    ax.imshow(pca_inversed[i].reshape(8, 8), cmap=plt.cm.gray_r, interpolation='nearest')
plt.tight_layout()
plt.show() 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is an example of the original data:
&lt;img src=&quot;/assets/img/digits0_9_original.png&quot; width=&quot;800&quot; height=&quot;180&quot; /&gt;
Here is that same example seen directly above, but this figure is the inverse transform of the data:
&lt;img src=&quot;/assets/img/digits0_9_pca.png&quot; width=&quot;800&quot; height=&quot;180&quot; /&gt;
The original 64 dimensions were reduced to 2. While PCA has reduced the dimensionality of the data by a factor of 32, the reconstructed images contain enough information that one might visually recognize the individual digits in the figure. Although it doesn’t seem that much of the variance is retained (~22%) within the first two principal components, it does appear that PCA reduction works for this dataset.&lt;/p&gt;

&lt;h3 id=&quot;t-distributed-stochastic-neighbor-embedding&quot;&gt;T-distributed Stochastic Neighbor Embedding&lt;/h3&gt;
&lt;p&gt;T-distributed Stochastic Neighbor Embedding (t-SNE) is a popular dimensionality reduction that helps identify patterns in data. This technique visualizes high-dimensional data via giving each individual data point a location within a two or three-dimensional map (&lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&quot;&gt;van der Maaten &amp;amp; Hinton 2008&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;One of the main advantages of using this algorithm is its ability to preserve local structure.  This basically means that points which are close to one another in the high-dimensional dataset will likely be close to one another in the map.&lt;/p&gt;

&lt;p&gt;The t-SNE algorithm undergoes two main stages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First, the algorithm models the probability distribution of neighbors around each point (‘neighbors’ refers to a set of points that are closest to each point). Similar points have a high probability of being picked whereas dissimilar points have a small probability of being picked.&lt;/li&gt;
  &lt;li&gt;Second, the algorithm defines a similar probability distribution over the points in the low-dimensional map. And it minimizes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;Kullback–Leibler divergence&lt;/a&gt; between the two distributions with respect to the locations of the points in the map. The Kullback-Leibler divergence (KL divergence) is a measure of how one probability distribution diverges from a second, expected probability distribution. So, it tries to minimize the difference between the similarities in higher-dimensional and lower-dimensional space for a representation of data points in lower-dimensional space.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With just using t-SNE on the dataset, it appears that the images corresponding to the different digits have been separated into different clusters of points. A clustering algorithm might select the separate clusters from this and one could possible assign new points to a label. While t-SNE plots often seem to display clusters, the visual clusters can be influenced by the selected parameterization and thus an understanding of the parameters for t-SNE is needed. These ‘clusters’ can be shown to appear in non-clustered &lt;a href=&quot;https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne/264647#264647&quot;&gt;data&lt;/a&gt; and therefore may not be accurate. Investigation may be needed to choose parameters and &lt;a href=&quot;https://distill.pub/2016/misread-tsne/&quot;&gt;validate&lt;/a&gt; the results. To illustrate this, the n_iter parameter (maximum number of iterations for the optimization) for the algorithm is set at 1000 here:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# t-SNE reduced data
n_iter = 1000
n_perplexity = 40

t_sne = TSNE(n_components=2, perplexity=n_perplexity, n_iter=n_iter)
tsne_result = t_sne.fit_transform(data) # This is t-SNE reduced data

# Plot of t-SNE reduced data 
plot_dim_red(tsne_result, data, 't-SNE reduced handwritten digits')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/t_sne_result.png&quot; width=&quot;650&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Whereas, the n_iter parameter is set at 10000 here:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# t-SNE reduced data
n_iter = 10000
n_perplexity = 40

t_sne = TSNE(n_components=2, perplexity=n_perplexity, n_iter=n_iter)
tsne_result = t_sne.fit_transform(data) # This is t-SNE reduced data

# Plot of t-SNE reduced data 
plot_dim_red(tsne_result, data, 't-SNE reduced handwritten digits')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/t_sne_result10000.png&quot; width=&quot;650&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the figure below shows perplexity values in ranges varying from (2 to 100). Some of the plots show the clusters, although with very different shapes. The plot with the perplexity equal to 2 shows merged clusters, so it may be better to have a higher perplexity.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Figure of multiple plots of different perplexities
perplex_lst = (2,5,30,50,100)
fig = plt.figure(figsize=(10,2.2))
plt_index = 0
for i in perplex_lst:
    plt_index = plt_index + 1
    t_sne = TSNE(n_components=2, perplexity=i, n_iter=n_iter)
    tsne_result = t_sne.fit_transform(data) # This is t-SNE reduced data
    ax = fig.add_subplot(1, 5, plt_index)
    ax.scatter(tsne_result[:, 0], tsne_result[:, 1],
                c=digits.target, edgecolor='none', alpha=0.5,
                cmap=plt.cm.get_cmap('tab10', 10), marker='.') 
    ax.set_title('perplexity = {}'.format(i))
    ax.set_yticklabels([]) # Turn off y tick labels
    ax.set_xticklabels([]) # Turn off x tick labels
    ax.set_yticks([]) # Turn off y ticks
    ax.set_xticks([]) # Turn off x ticks
plt.tight_layout()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/t_sne_multi_perplexity.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For this demonstration, t-SNE appears to have found structure in this dataset. From now on, unless otherwise stated, results from 1,000 iterations and a perplexity of 40 will be shown when using t-SNE dimensionality reduction.&lt;/p&gt;

&lt;h3 id=&quot;truncated-singular-value-decomposition-truncated-svd&quot;&gt;Truncated singular value decomposition (truncated SVD)&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html&quot;&gt;truncated singular value decomposition&lt;/a&gt; estimator performs linear dimensionality reduction. Singular value decomposition (&lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD&quot;&gt;SVD&lt;/a&gt;) produces a factorization of a matrix, in which the number of columns is equal to the specified truncation. And matrix decomposition (or matrix factorization) involves describing a given matrix using its constituent elements. For (&lt;a href=&quot;https://www.oreilly.com/library/view/scikit-learn-cookbook/9781783989485/ch01s13.html&quot;&gt;example&lt;/a&gt;), given an n x n matrix, SVD would produce matrices with n columns. However, &lt;a href=&quot;https://link.springer.com/article/10.1007/BF01937276&quot;&gt;Truncated SVD&lt;/a&gt; would produce matrices with the specified number of columns, thereby reducing the dimensionality. One of the advantages in using this method is that it can operate on &lt;a href=&quot;https://en.wikipedia.org/wiki/Sparse_matrix&quot;&gt;sparse matrices&lt;/a&gt;. Let’s fit the model and examine the results:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Truncated SVD reduced data
svd = TruncatedSVD(n_components=2, n_iter=10)
svd_result = svd.fit_transform(data)  

# Plot of Truncated SVD reduced data 
plot_dim_red(svd_result, data, 'Truncated SVD reduced handwritten digits')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Truncated_SVD_result.png&quot; width=&quot;650&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It appears that results from this transformer are similar to results from using PCA above.&lt;/p&gt;

&lt;h3 id=&quot;isometric-mapping&quot;&gt;Isometric Mapping&lt;/h3&gt;
&lt;p&gt;Isometric Mapping (&lt;a href=&quot;https://web.mit.edu/cocosci/Papers/sci_reprint.pdf&quot;&gt;isomap&lt;/a&gt;) is a non-linear dimensionality reduction method that uses local metric information to learn the underlying geometry of a dataset. It provides a method for estimating the geometry of a data manifold based on a rough estimate of each data point’s neighbors on the manifold. This method combines some of the features of PCA and Multidimensional scaling (&lt;a href=&quot;https://en.wikipedia.org/wiki/Multidimensional_scaling&quot;&gt;MDS&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This algorithm has three &lt;a href=&quot;https://web.mit.edu/cocosci/Papers/nips02-localglobal-in-press.pdf&quot;&gt;stages&lt;/a&gt;. It starts by determining a neighborhood graph for the data points. Next, it computes the geodesic &lt;a href=&quot;https://en.wikipedia.org/wiki/Distance_(graph_theory)&quot;&gt;distance&lt;/a&gt; for all pairs of data points. Finally, through applying MDS to the shortest-path distance matrix, it constructs the low dimensional embedding of the data in Euclidean space. This is what the algorithm produces with this dataset:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Isomap reduced data
iso = Isomap(n_components=2)
iso_result = iso.fit_transform(data)

# Plot of isomap reduced data 
plot_dim_red(iso_result, data, 'Isomap reduced handwritten digits')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/isomap_result.png&quot; width=&quot;650&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(A PY file with more code (Python) is available on this &lt;a href=&quot;https://github.com/sunshinescience/dim_red_cluster/blob/master/dim_red_cl.py&quot;&gt;github repo&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;clustering&quot;&gt;Clustering&lt;/h2&gt;
&lt;p&gt;One of the many ways to try to understand data is to organize it into groups.  Clustering is a very popular unsupervised learning technique that is used to find logical groupings within data. Clustering doesn’t use any Y variables or labels on the data. It investigates the data structure itself. And the goal of clustering is to minimize inter-cluster similarity and maximize intra-cluster similarity.&lt;/p&gt;

&lt;p&gt;If you have a large input data set, you can find patterns within the data via clustering. For example, you may want to use a clustering algorithm to fit a large variety of emails into individual topics. These could include topics such as science, technology, or personal emails, and etc. That is one example of clustering, but it applies to so many use cases.&lt;/p&gt;

&lt;p&gt;Scikit-learn has the following clustering algorithms: K-means, Mini Batch K-Means, Affinity propagation, Mean-shift, Spectral clustering, Ward hierarchical clustering, Agglomerative clustering, DBSCAN, Gaussian mixtures, and Birch. There is an overview of the different clustering methods on scikit-learn, found &lt;a href=&quot;https://scikit-learn.org/stable/modules/clustering.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For the handwritten digits dataset, the following clustering techniques will be used: k-means, mean-shift, spectral, DBSCAN, and affinity propagation clustering. These methods were chosen to try on this dataset because k-means clustering is one of the most popular machine learning algorithms that allows us to maximize inter-cluster similarity and minimize intra-cluster similarity. And mean-shift clustering is very similar to k-means clustering, but instead of using a sum of neighborhood points, the kernel function in mean-shift clustering would apply a probability weighted sum of points. Spectral and affinity propogation clustering  differ from k-means in that they can be used for a dataset with non-flat geometry and graph distance (e.g., nearest-neighbor graph) metrics are used. DBSCAN differs from k-means as it too can be used for a dataset with non-flat geometry, but the distances between nearest points metrics are used. The main goal here is to assess a few of the dimensionality reduction methods and a few of the clustering techniques on the handwritten digits dataset, for comparison purposes.&lt;/p&gt;

&lt;p&gt;Let’s import some more useful libraries:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from sklearn.cluster import KMeans
from sklearn.cluster import MeanShift
from sklearn.cluster import estimate_bandwidth
from sklearn.cluster import SpectralClustering
from sklearn.cluster import DBSCAN
from sklearn.cluster import AffinityPropagation

# Import figures and statistics 
import figures
import statistics
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;k-means-clustering&quot;&gt;K-means clustering&lt;/h3&gt;
&lt;p&gt;The goal of &lt;a href=&quot;http://www.labri.fr/perso/bpinaud/userfiles/downloads/hartigan_1979_kmeans.pdf&quot;&gt;k-means&lt;/a&gt; clustering is to minimize the sum of squared distances between all points and the cluster centre (&lt;a href=&quot;https://pdfs.semanticscholar.org/0ec1/32fce9971d1e0e670e650b58176dc7bf36da.pdf&quot;&gt;Ray &amp;amp; Turi 1999&lt;/a&gt;). Let’s use the demo on this &lt;a href=&quot;https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py&quot;&gt;site&lt;/a&gt; as a guide to get started. The demo compares different initialization strategies for k-means clustering. Different initialization strategies will be assessed here and the k-means algorithm from &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html&quot;&gt;scikit-learn&lt;/a&gt; will be utilized.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Metrics for K-means clustering

n_clusters = n_digits # The number of clusters
n_init = 10
sample_size = 300

# Metrics to evaluate the model
print('init\t\t homo\t compl\t v-meas\t ARI\t AMI\t F-M\t silhouette')

def kmeans_metrics(estimator, name, data):
    &quot;&quot;&quot;
    Compare different initializations of K-means to assess the quality of the clustering.

    Parameters:
        estimator: K-means algorithm with parameters to pass (init, n_clusters, and n_init)
        name: name of the method for initialization
        data: data
    &quot;&quot;&quot;
    estimator.fit(data)
    print ('{:&amp;lt;9}\t {:.2f}\t {:.2f}\t {:.2f}\t {:.2f}\t {:.2f}\t {:.2f}\t {:.2f}'
    .format(name, metrics.homogeneity_score(labels, estimator.labels_),
    metrics.completeness_score(labels, estimator.labels_),
    metrics.v_measure_score(labels, estimator.labels_),
    metrics.adjusted_rand_score(labels, estimator.labels_),
    metrics.adjusted_mutual_info_score(labels,  estimator.labels_),
    metrics.fowlkes_mallows_score(labels, estimator.labels_),
    metrics.silhouette_score(data, estimator.labels_,
                                metric='euclidean',
                                sample_size=sample_size)))

kmeans_metrics(KMeans(init='k-means++', n_clusters=n_clusters, n_init=10),
            name=&quot;k-means++&quot;, data=data)

kmeans_metrics(KMeans(init='random', n_clusters=n_clusters, n_init=10),
            name=&quot;random&quot;, data=data)

# In this case the seeding of the centers is deterministic, thus the kmeans algorithm is run only once (i.e., n_init=1)
pca = PCA(n_components=n_digits).fit(data)
kmeans_metrics(KMeans(init=pca.components_, n_clusters=n_clusters, n_init=1),
            name=&quot;PCA-based&quot;,
            data=data)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Initially, a &lt;em&gt;k&lt;/em&gt; (number of clusters) value as a parameter needs to be provided when using the k-means algorithm to partition clusters. Here, the number of digits equals the number of clusters (see above code). Let’s perform k-means clustering on the dimensionality reduced data.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# K-means clustering

def k_means_reduced(reduced_data, initialization, n_clusters, n_init):
    &quot;&quot;&quot;
    This returns K-means clustering on data that has undergone dimensionality reduction.
    Parameters:
        reduced_data: The data that has undergone dimensionality reduction
        initialization: Method for initialization, defaults to ‘k-means++’:
        n_clusters: The number of clusters to form as well as the number of centroids to generate.
        n_init: Number of times the k-means algorithm will run with different centroid seeds.
    &quot;&quot;&quot;
    k_means = KMeans(init=initialization, n_clusters=n_clusters, n_init=n_init) 
    k_means_model = k_means.fit(reduced_data)
    return k_means_model

# K-means clustering on PCA reduced data
k_pca = k_means_reduced(pca_result, 'k-means++', n_clusters, n_init)
# K-means clustering on t-SNE reduced data
k_t_sne = k_means_reduced(tsne_result, 'k-means++', n_clusters, n_init)
# K-means clustering on Truncated SVD reduced data
k_SVD = k_means_reduced(svd_result, 'k-means++', n_clusters, n_init)
# K-means clustering on isomap reduced data
k_iso = k_means_reduced(iso_result, 'k-means++', n_clusters, n_init)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, the results of k-means clustering are visualized:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Figure of PCA vs t-SNE reduced data and K-means clustering on both
fig, axarr = plt.subplots(2, 2, figsize=(10,8)) 

# Plot of PCA reduced data 
axarr[0, 0].scatter(pca_result[:,0],pca_result[:,1],c='k')
axarr[0, 0].set_title('PCA reduced data')

# Plot of t-SNE reduced data 
axarr[1, 0].scatter(tsne_result[:,0], tsne_result[:,1],c='k')
axarr[1, 0].set_title('t-SNE reduced data')

# Plot of K-means clustering on PCA reduced data
cluster_ax = axarr[0, 1]
cluster_ax.scatter(pca_result[:,0],pca_result[:,1],c=k_pca.labels_, marker='o', edgecolor='none', alpha=0.5)
centroids = k_pca.cluster_centers_
centroid_x = cluster_ax.scatter(centroids[:, 0], centroids[:, 1],
            marker='o', edgecolors='k', 
            c=np.arange(n_clusters), zorder=10)
cluster_ax.set_title('K-means clustering on PCA reduced data')

# Plot of K-means clustering on t-SNE reduced data
cluster_ax2 = axarr[1, 1]
cluster_ax2.scatter(tsne_result[:,0],tsne_result[:,1],c=k_t_sne.labels_, marker='o', edgecolor='none', alpha=0.5)
centroids = k_t_sne.cluster_centers_
centroid_x = cluster_ax2.scatter(centroids[:, 0], centroids[:, 1],
            marker='o', edgecolors='k', 
            c=np.arange(n_clusters), zorder=10) # If you want the cluster center marker as an 'x', use: marker='x', s=100, linewidths=5, color='k', zorder=10
cluster_ax2.set_title('K-means clustering on t-SNE reduced data')
fig.suptitle('K-means clustering on the handwritten digits dataset')
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pca_tsne_k_means.png&quot; /&gt;
The above figure compares k-means clustering on PCA dimensionality reduced data vs t-SNE dimensionality reduced data. The black-lined circles on the right two plots indicate the location of the center of each corresponding cluster. The individual plots for k-means clustering on PCA, t-SNE, truncated SVD, and Isomap dimensionality reduced data are as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Plot of k-means clustering on t-SNE reduced data
plot_dim_red_clust(tsne_result, k_t_sne, data, 't-SNE', 'K-means', centroids=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/t-SNE_K-means_plot.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Plot of k-means clustering on pca reduced data
plot_dim_red_clust(pca_result, k_pca, data, 'PCA', 'K-means', centroids=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/PCA_K-means_plot.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Plot of K-means clustering on isomap reduced data
plot_dim_red_clust(iso_result, k_iso, data, 'isomap', 'K-means')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/isomap_K-means_plot.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Plot of K-means clustering on truncated SVD reduced data
plot_dim_red_clust(svd_result, k_SVD, data, 'truncated SVD', 'K-means')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/truncated SVD_K-means_plot.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In some cases, the digits have similar visual features, so it is difficult for the model to identify which cluster they belong to. This happens more frequently for the digit 8 cluster. For example, the digit 8 and the digit 1 often get clustered together. The figure below shows images that were clustered in the same cluster, with their corresponding label written above to show which digit they really are:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/four_digits1188.png&quot; width=&quot;500&quot; height=&quot;120&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Whereas for the digit 0, after running the model multiple times, the 0 digit is clustered together accurately, without other digits within that cluster. See the digit 0 examples below that are clustered together:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/four_digits0000.png&quot; width=&quot;500&quot; height=&quot;120&quot; /&gt;&lt;/p&gt;

&lt;p&gt;See another convoluted example in the figure below, which are clustered in the same cluster:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/four_digits5759.png&quot; width=&quot;500&quot; height=&quot;120&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The number of incorrect digits in each cluster can be found using the following code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Obtaining information about how well the digits were clustered from k-means clustering:
labels = digits.target

def cluster_digit_accuracy(labels_array, labels):
    sampl_lst = []
    digit_lst = []
    wher_lst = []
    wher_dict = {} # keys are digits, values are the sample numbers of digits that do not match the overall cluster digit (mode), for each cluster
    len_dict = {} # keys are digits, values are the total amount of digits in the corresponding cluster
    for i in range(10):
        cl_num = i
        cl_data = np.where(labels_array == cl_num)[0]
        clust_mode = statistics.mode(labels[cl_data])
        sampl_lst.append(cl_data)
        digit_lst.append(labels[cl_data])
        not_equal = (np.where(labels[cl_data] != clust_mode)[0]).tolist()
        wher_dict[clust_mode] = not_equal
        wher_lst.append(not_equal)
        cl_len = len(labels[cl_data])
        len_dict[i] = cl_len
    print ('The overall digit of each cluster and the total amount of digits in each cluster are: ', len_dict)

    non_match_dict = {}
    for j in range(10):
        non_match_dict[j] = len(wher_dict[j])
    print ('The overall digit (mode) and the amount of digits that do not match the mode of each cluster are: ', non_match_dict) # Prints the digit number as the key and the corresponding value is the amount of wrong digits within that particular cluster. 

cluster_digit_accuracy(k_t_sne.labels_, labels)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Information from the clusters achieved via k-means clustering is shown in the table below. For an example run, the digit number (i.e., the mode) of each cluster is shown in the left column. The middle column shows the amount of digits within each cluster. The right column shows the amount of wrong digits assigned to that cluster. So, in a cluster with the digit zero, all of the digits are zero. The digit zero is clustered correctly in this run as there are 178 zeroes in the dataset. Whereas in a cluster with the digit one, there are thirty-seven digits that are not the number one within that cluster, and so on. This code may need more work to get this assessment more accurate, but for these purposes we will just assess the clusters in this manner.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Digit&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Total digits in cluster&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Amount of incorrect digits in cluster&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;178&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;235&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;37&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;174&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;31&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;151&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;182&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;182&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;197&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;187&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;173&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;71&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;138&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It appears that t-SNE discovered some structure in the data and the digits have been separated into different clusters of points via k-means clustering. The clustering algorithms below might select the separate clusters from the t-SNE dimensionality reduced data and assign points to labels. For the following clustering algorithms, t-SNE will be the dimensionality reduction method used on the dataset for comparison purposes to the k-means clustering algorithm.&lt;/p&gt;
&lt;h3 id=&quot;mean-shift-clustering&quot;&gt;Mean shift clustering&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=513076&quot;&gt;mean shift&lt;/a&gt; algorithm is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Nonparametric_statistics&quot;&gt;nonparametric&lt;/a&gt; clustering technique. The method does not constrain the shape of the clusters. One of the advantages of &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.8968&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;mean shift&lt;/a&gt; over k-means is that the number of clusters is not pre-specified, because mean shift is likely to find only a few clusters if only a small number exist. However, mean shift can be much slower than k-means, and still requires initial selection of a bandwidth parameter.&lt;/p&gt;

&lt;p&gt;Mean shift builds upon the concept of kernel density estimation (&lt;a href=&quot;https://en.wikipedia.org/wiki/Kernel_density_estimation&quot;&gt;KDE&lt;/a&gt;). The algorithm works by initially defining the bandwidth of the kernel function. It places this on data points and calculates the mean for all the points in the bandwidth of the kernel. Next, it moves the center of the bandwidth of the kernel to the location of that mean. It continues until there is convergence.&lt;/p&gt;

&lt;p&gt;Let’s apply the clustering &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html&quot;&gt;algorithm&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Mean-shift clustering
# Use the estimate_bandwidth function to estimate a good bandwidth for the data
bandwidth = round(estimate_bandwidth(data))

mean_s = MeanShift(bandwidth=bandwidth)
mean_s.fit(pca_result)

ms = MeanShift(bandwidth=bandwidth)
ms_tsne = ms.fit(tsne_result)
ms_labels = ms_tsne.labels_
ms_cluster_centers = ms_tsne.cluster_centers_

ms_labels_unique = np.unique(ms_labels)
ms_n_clusters = len(ms_labels_unique)

print ('The number of estimated clusters from mean-shift clustering is: {}'.format(ms_n_clusters))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The results provide a varying number of clusters over several runs (from 11 to 13 usually). The expected number of clusters (10) are not achieved with mean shift clustering for this dataset. The plot below illustrates twelve clusters. The results of mean shift clustering are visualized as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Visualize the results of Mean-shift clustering
color = ms_labels
fig, axarr = plt.subplots(1, 2, figsize=(9,4))  
# Plot of t-SNE reduced data 
ax1 = axarr[0]
ax1.scatter(tsne_result[:,0], tsne_result[:,1], c='k', marker='.')
ax1.set_title('t-SNE reduced data')

# Plot of mean shift clustering on t-SNE reduced data
ax2 = axarr[1]
ax2.scatter(tsne_result[:,0], tsne_result[:,1], c=color, marker='.')
cluster_center = ms_cluster_centers[ms_labels]
ax2.scatter(cluster_center[:,0], cluster_center[:,1],
            marker='o', edgecolors='k', 
            c=color, zorder=10) 
ax2.set_title('Mean-shift clustering on t-SNE reduced data')
plt.tight_layout()
plt.show()

# Plot of mean-shift clustering on t-SNE reduced data
plot_dim_red_clust(tsne_result, ms_tsne, data, 't-SNE', 'Mean-shift')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/t-SNE_Mean-shift_plot.png&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;spectral-clustering&quot;&gt;Spectral clustering&lt;/h3&gt;
&lt;p&gt;Spectral clustering is considered an exceptional graph clustering technique. The method can ignore sparse interconnections between arbitrarily shaped clusters of data. This approach can be used to identify communities of nodes in a graph, based on the edges connecting them. To perform spectral clustering, three main &lt;a href=&quot;https://towardsdatascience.com/spectral-clustering-for-beginners-d08b7d25b4d8&quot;&gt;steps&lt;/a&gt; are involved:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Create a similarity graph between the N objects to cluster&lt;/li&gt;
  &lt;li&gt;Compute the first k eigenvectors of its Laplacian matrix, in order to define a feature vector for each object&lt;/li&gt;
  &lt;li&gt;Run k-means on these features, in order to separate objects into k classes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An in depth tutorial can be found &lt;a href=&quot;http://people.csail.mit.edu/dsontag/courses/ml14/notes/Luxburg07_tutorial_spectral_clustering.pdf&quot;&gt;here&lt;/a&gt;. This paper discussed two versions of normalized spectral clustering. It has been suggested that spectral clustering often outperforms k-means clustering. Let’s see what it does to this dataset compared to k-means clustering. Here, &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html&quot;&gt;this&lt;/a&gt; algorithm from scikit-learn is used and the result is visualized:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Spectral clustering
# sc_result = SpectralClustering(n_clusters=n_clusters, assign_labels=&quot;discretize&quot;).fit(data)
sc = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors',
                        assign_labels='kmeans')  
sc_tsne = sc.fit(tsne_result)                
sc_labels = sc.fit_predict(tsne_result)

# Visualize the results of spectral clustering
plot_dim_red_clust(tsne_result, sc_tsne, data, 't-SNE', 'Spectral')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/t-SNE_Spectral_plot.png&quot; /&gt;
Similar to k-means, the results of spectral clustering show that ten clusters were obtained using this method.&lt;/p&gt;
&lt;h3 id=&quot;dbscan-clustering&quot;&gt;DBSCAN clustering&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# DBSCAN clustering
db = DBSCAN(eps=3, min_samples=2)
db_tsne = db.fit(tsne_result)

# Visualize the results of DBSCAN clustering
plot_dim_red_clust(tsne_result, db_tsne, data, 't-SNE', 'DBSCAN')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/t-SNE_DBSCAN_plot.png&quot; /&gt;
The results of DBSCAN clustering show that more clusters than expected were obtained using this method.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-further-study&quot;&gt;Conclusion and further study&lt;/h2&gt;

&lt;p&gt;In this demonstration, various dimensionality reduction and clustering methods were investigated on the handwritten digits dataset. When working with large data sets with many features, dimensionality reduction can be an important technique to improve the subsequent clustering process. Instead of using many features, one can use the features that are the most significant which represent the underlying data. And clustering is an important method that helps identify patterns in the underlying data.&lt;/p&gt;

&lt;p&gt;Within scientific fields that deal with empirical data, it may be helpful to identify trends and patterns (e.g., groups or &lt;em&gt;clusters&lt;/em&gt;) in the data. It appears that for these analyses on the handwritten digits dataset, the dimensionality reduction method that worked best was t-SNE and the clustering algorithms that were the most promising were k-means and spectral clustering.&lt;/p&gt;

&lt;p&gt;To take this study further, these methods will be used on high-dimensional &lt;em&gt;unlabelled&lt;/em&gt; data.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">A Brief Introduction to Object Oriented Programming</title><link href="http://localhost:4000/2019/04/25/object-oriented-programming.html" rel="alternate" type="text/html" title="A Brief Introduction to Object Oriented Programming" /><published>2019-04-25T00:00:00+01:00</published><updated>2019-04-25T00:00:00+01:00</updated><id>http://localhost:4000/2019/04/25/object-oriented-programming</id><content type="html" xml:base="http://localhost:4000/2019/04/25/object-oriented-programming.html">&lt;h2 id=&quot;what-is-object-oriented-programming&quot;&gt;What is Object Oriented Programming?&lt;/h2&gt;
&lt;p&gt;Object oriented programming (OOP) is a pattern of programming in which the solution to a programming problem is modelled as a collection of collaborating objects.&lt;/p&gt;

&lt;p&gt;Prior to the development of OOP, procedural programming divided a program into a set of functions. The data was stored in variables and the functions operated on that data. It was relatively simple, however, complications would arise when functions were changed. For example, another function may depend on the function that was changed, which could lead to the code becoming difficult to maintain. Essentially, OOP came about to help with this and other programming problems. Some benefits of using OOP include: reducing complexity, isolating the impact of changes made to code, eliminating redundant code, and refactoring code (e.g., condensing lengthy if/else statements). It also reduces development time by supporting programming by customization.&lt;/p&gt;

&lt;h4 id=&quot;why-use-oop-in-python&quot;&gt;Why use OOP in Python?&lt;/h4&gt;
&lt;p&gt;In Python, you could use OOP to build a data type(s) that matches real world objects. You can re-use and build upon code. You can work with abstract objects related to your domain.&lt;/p&gt;

&lt;p&gt;Python is an object oriented language. For example, its class model supports polymorphism, operator overloading, and multiple inheritance (see: &lt;a href=&quot;http://shop.oreilly.com/product/0636920028154.do&quot;&gt;Learning Python&lt;/a&gt; by Mark Lutz). But OOP is an option in Python, you don’t need to become an OOP expert and immediately start applying it because Python supports a procedural programming mode as well.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-four-core-principles-of-oop&quot;&gt;What are the four core principles of OOP?&lt;/h2&gt;
&lt;p&gt;It is often said that there are four main concepts in OOP, which are: encapsulation, abstraction, inheritance, and polymorphism. 
I generally try to find a mnemonic way to remember things. What came to mind when I first learned this was, apple (a from apple is the first letter in &lt;em&gt;abstraction&lt;/em&gt;) pie, or even ‘a pie.’&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A - abstraction&lt;/li&gt;
  &lt;li&gt;P - polymorphism&lt;/li&gt;
  &lt;li&gt;I  - inheritance&lt;/li&gt;
  &lt;li&gt;E – encapsulation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While it worked for me to remember these concepts, lets look at these concepts in a logical order to better understand them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encapsulation&lt;/strong&gt; – It wraps up the data and code into a single unit (i.e., a class). This keeps the data and code safe from outside interference and misuse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstraction&lt;/strong&gt; – This can be thought of as an extension of encapsulation. Abstraction is the process of hiding unnecessary details from the user. Essentially, it hides the complexity and provides only essential information to the user.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inheritance&lt;/strong&gt; – It’s the process of creating a new object or class (called a derived class or subclass or child class) from an existing object or class (called a base class or super class or parent class), while retaining similar implementation. The derived class inherits all of the capabilities of the base class, but it can be extended and refined. Basically, the derived class is a more specialized one and it provides reusability. There are multiple types of inheritance, which include: single, multiple, multi-level, hierarchical, and hybrid. These types are based on the paradigm and specific language.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Polymorphism&lt;/strong&gt; – The word means ‘having many forms’. It refers to the ability to process objects differently depending on their data type or class (I really like this definition found here: &lt;a href=&quot;https://www.webopedia.com/TERM/P/polymorphism.html&quot;&gt;Polymorphism&lt;/a&gt;). One is able to redefine methods for derived classes. It essentially describes the concept that objects of different types can be accessed through the same interface. And each type can provide its own, independent implementation of this interface.&lt;/p&gt;

&lt;p&gt;These OOP concepts can help our work become organized, modular and maintainable. They help us to omit repetition, keep data safe, and make our work user friendly.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-solid-principles&quot;&gt;What are the SOLID Principles?&lt;/h2&gt;
&lt;p&gt;SOLID is an acronym for five class design principles that are intended to help a software design become more understandable, flexible, and easier to maintain. SOLID can be used as a checklist, something worth revisiting to help you create a well-made class design, but each part may not be necessary to implement.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/solid.png&quot; width=&quot;500&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Robert C. Martin (known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Robert_C._Martin&quot;&gt;“Uncle Bob”&lt;/a&gt;) defined principles of OOD, which can be found here: &lt;a href=&quot;http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod&quot;&gt;SOLID principles&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;single-responsibility-principle-srp&quot;&gt;&lt;strong&gt;Single responsibility principle&lt;/strong&gt; (&lt;a href=&quot;https://drive.google.com/file/d/0ByOwmqah_nuGNHEtcU5OekdDMkk/view&quot;&gt;SRP&lt;/a&gt;)&lt;/h4&gt;
&lt;p&gt;“A class should have one, and only one, reason to change.”&lt;/p&gt;

&lt;p&gt;An object/class should have one primary responsibility. The object should have one reason to exist and that reason should be encapsulated within the class. It can have different behaviors, but those behaviors are oriented around that sole reason for existence. One may not want to create god objects that try to do too many different things.&lt;/p&gt;

&lt;h4 id=&quot;openclosed-principle-ocp&quot;&gt;&lt;strong&gt;Open/closed principle&lt;/strong&gt; (&lt;a href=&quot;https://drive.google.com/file/d/0BwhCYaYDn8EgN2M5MTkwM2EtNWFkZC00ZTI3LWFjZTUtNTFhZGZiYmUzODc1/view&quot;&gt;OCP&lt;/a&gt;)&lt;/h4&gt;
&lt;p&gt;“You should be able to extend a classes behavior, without modifying it.”&lt;/p&gt;

&lt;p&gt;A system should be open for extension, but closed for modification. For example, if you want to add a feature, you could do this by writing new code, without modifying the existing code. Robert Martin provides an updated definition of the initial one provided by (Meyer 1988).&lt;/p&gt;

&lt;h4 id=&quot;liskov-substitution-principle-lsp&quot;&gt;&lt;strong&gt;Liskov substitution principle&lt;/strong&gt; (&lt;a href=&quot;https://drive.google.com/file/d/0BwhCYaYDn8EgNzAzZjA5ZmItNjU3NS00MzQ5LTkwYjMtMDJhNDU5ZTM0MTlh/view&quot;&gt;LSP&lt;/a&gt;)&lt;/h4&gt;
&lt;p&gt;“Derived classes must be substitutable for their base classes.”&lt;/p&gt;

&lt;p&gt;Objects in a program should be replaceable with instances of their subclasses (or child classes or derived classes, whichever term you use for them) without altering the correctness of the program. So, you should be able to treat the child classes like their parent classes. As Robert Martin states, “Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.”&lt;/p&gt;

&lt;h4 id=&quot;interface-segregation-principle-isp&quot;&gt;&lt;strong&gt;Interface segregation principle&lt;/strong&gt; (&lt;a href=&quot;https://drive.google.com/file/d/0BwhCYaYDn8EgOTViYjJhYzMtMzYxMC00MzFjLWJjMzYtOGJiMDc5N2JkYmJi/view&quot;&gt;ISP&lt;/a&gt;)&lt;/h4&gt;
&lt;p&gt;“Make fine grained interfaces that are client specific.”&lt;/p&gt;

&lt;p&gt;Many specific interfaces are better than one general purpose interface. The idea of the interface here is having formalized lists of method names that a class can choose to support. And those lists of methods should be as small as possible and if they start to get bigger, then they should be split up into smaller interfaces as classes can choose to implement multiple smaller interfaces. A class should not have to support enormous lists of methods that it doesn’t need.&lt;/p&gt;

&lt;h4 id=&quot;dependency-inversion-principle-dip&quot;&gt;&lt;strong&gt;Dependency Inversion Principle&lt;/strong&gt; (&lt;a href=&quot;https://drive.google.com/file/d/0BwhCYaYDn8EgMjdlMWIzNGUtZTQ0NC00ZjQ5LTkwYzQtZjRhMDRlNTQ3ZGMz/view&quot;&gt;DIP&lt;/a&gt;)&lt;/h4&gt;
&lt;p&gt;“Depend on abstractions, not on concretions.”&lt;/p&gt;

&lt;p&gt;Depend on abstractions, not on concretions. This is the idea of not directly tying concrete objects together, but making them deal with abstractions in order to minimize dependencies between our objects. For example, if you have a high-level object, which depends on two low-level objects - you disconnect these two very concrete classes (i.e., remove the two low-level objects that are very specific), so you remove those connections and then insert a new layer between them, which might be abstract classes that are more basic. Therefore, it allows substitutions and flexibility. You can replace and extend the object.&lt;/p&gt;

&lt;p&gt;Using some or all of the above principles can help keep your code maintainable, testable, and it can reduce the complexity. That’s a win-win result in my book!&lt;/p&gt;</content><author><name></name></author><summary type="html">What is Object Oriented Programming? Object oriented programming (OOP) is a pattern of programming in which the solution to a programming problem is modelled as a collection of collaborating objects.</summary></entry></feed>