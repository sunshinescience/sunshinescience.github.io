<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-04-27T14:01:57+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Science to Technology</title><subtitle>Welcome to &quot;S2T.&quot; This is a blog that provides my personal journey as a scientist trying to transition into the tech world. Read and see lessons learned thus far. I hope you can find tools and resources that may help you enter/transition into an exciting field!</subtitle><entry><title type="html">Dimensionality Reduction and Clustering on the Handwritten Digits Dataset</title><link href="http://localhost:4000/2019/04/27/dimensionality-reduction-clustering.html" rel="alternate" type="text/html" title="Dimensionality Reduction and Clustering on the Handwritten Digits Dataset" /><published>2019-04-27T00:00:00+01:00</published><updated>2019-04-27T00:00:00+01:00</updated><id>http://localhost:4000/2019/04/27/dimensionality-reduction-clustering</id><content type="html" xml:base="http://localhost:4000/2019/04/27/dimensionality-reduction-clustering.html">&lt;p&gt;&lt;img src=&quot;/assets/img/gear_head_concept.jpg&quot; width=&quot;500&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One of the goals of science is the pursuit of knowledge. And one of the goals of technology is to create products that solve problems. As technology appears to be the practical application of science, I think that applying machine learning (ML) techniques is exciting! ML is a ubiquitous technology –it is found everywhere! Just a few applications include the type of product a person might buy, the ability for cars to drive themselves, determining if an email is spam and moving it into your spam folder, and the list goes on…&lt;/p&gt;

&lt;p&gt;My interest in machine learning piqued as I learned what an incredible tool it can be for transforming information into knowledge. These techniques provide various ways to solve a problem.  Machine learning is a method of providing systems the ability to learn and improve from experience in an automated fashion, without the need for explicitly being programmed.  Two of the broad categories in which machine learning tasks are classified into are: supervised learning and unsupervised learning.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Unsupervised learning&lt;/strong&gt; essentially learns from unlabelled and uncategorized input data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Supervised learning&lt;/strong&gt; undertakes the task of learning a mapping function based on input-output pairs. This task learns the mapping function so well that it maps an input to predict an output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have found scikit-learn incredibly useful. Scikit-learn supports several categories of algorithms, such as regression, classification, clustering, and dimensionality reduction. It offers tools that allow you to find the best model for your data (model selection). The scikit-learn algorithm cheat-sheet can be found &lt;a href=&quot;https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html&quot;&gt;here&lt;/a&gt;. Moreover, scikit-learn has tools for data processing and preparation (&lt;a href=&quot;https://scikit-learn.org/stable/modules/preprocessing.html&quot;&gt;preprocessing&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;As a scientist, I really enjoyed finding patterns and structure in various forms of data. An interesting machine learning project to delve into may be clustering, as one of the goals of clustering is to identify structure in data. And dimensionality reduction might improve the clustering of the data. Thus, in this demonstration, dimensionality reduction and subsequent clustering methods will be explored. While I would like to apply this to high dimensional unlabelled data, I would initially like to work with a labelled dataset. First, I’ll assess the handwritten digits dataset, as it has been tagged with labels. I’ll use Python for this demonstration.&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;For this demonstration, I’ll use the handwritten digits dataset from scikit-learn. Scikit-learn has a copy of the test set of the UCI ML hand-written digits &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits&quot;&gt;dataset&lt;/a&gt;. An example to load and visualize the data via scikit-learn can be found &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html&quot;&gt;here&lt;/a&gt;. The dataset characteristics on scikit-learn can be found in section &lt;a href=&quot;https://scikit-learn.org/stable/datasets/index.html#digits-dataset&quot;&gt;5.2.4&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This dataset is made up of 1797 images of hand-written digits. There are ten classes, in which each class refers to a handwritten digit (0 to 9). See an example of digits 0 to 9 in the images below. The axes are the pixel numbers, for each 8x8 pixel image.
&lt;img src=&quot;/assets/img/digits_10example.png&quot; width=&quot;500&quot; height=&quot;250&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s first import a few Python libraries that are useful. Let’s also import some libraries from scikit-learn and load the dataset.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import load_digits
from sklearn.preprocessing import scale

# Load the handwritten digits dataset
digits = load_digits()

# Standardize a dataset along any axis 
data = scale(digits.data) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code will generate the above figure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Plot of ten example digits (from 0 to 9)
fig = plt.figure(figsize=(10,5))
plt_index = 0
for i in range(10):
    plt_index = plt_index + 1
    ax = fig.add_subplot(2, 5, plt_index)
    ax.imshow(digits.images[i], cmap=plt.cm.gray_r, interpolation='nearest')
plt.tight_layout()
plt.show() 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each individual label indicates which digit (0 to 9) is captured in that particular image. When plotting, each individual data point represents an 8x8 image of a digit. Each 8x8 image has been pre-processed such that the digit fits and is centered. Generally if an image is in grayscale, the tuple returned contains only the number of rows and columns, whereas a color image would have a tuple returned of rows, columns, and channels. In this example, these images are grayscaled as the tuple returned is 8x8 using the code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print (digits.images[0].shape)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Generally, the dimensionality in an image is the number of pixels within its image. In this case, each image consists of 64 pixels that represent the features of that particular digit. A feature can be thought of as an individual measureable property of something that is being observed. Thus, instead of thinking of each image as an 8x8 grid of pixels, we can consider it as a vector, or list, of 64 features.&lt;/p&gt;

&lt;p&gt;Here, I’ll show a plot of four of each of the nine digits in order to illustrate differences contributed by participants whom had written the digits. The ticks and corresponding tick labels have been removed for ease of view. See the plot and corresponding code below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Plot of four examples from each of the 10 digits (0 through 9)
sample_lst = [] # A list of lists. Each list within contains four sample numbers of a single digit, and it goes from digit 0 (the first list within the list of lists) to digit 9 (the last list)
for i in range(10):
    sample_nums = np.where(labels == i)[0][0:4] # Indexes (0:4) into an array of sample numbers that are a specified digit (the digit is written as i in the loop)
    sample_nums = sample_nums.tolist() # Convert the array called sample_nums to a list with the same items
    sample_lst.append(sample_nums) 
if 0:
    fig = plt.figure(figsize=(10,6))
    plt_index = 0
    for i in range(0,10):
        for j in sample_lst[i]:
            plt_index = plt_index + 1
            ax = fig.add_subplot(5, 8, plt_index)
            ax.imshow(digits.images[j], cmap=plt.cm.gray_r, interpolation='nearest')
            ax.set_yticklabels([]) # Turn off y tick labels
            ax.set_xticklabels([]) # Turn off x tick labels
            ax.set_yticks([]) # Turn off y ticks
            ax.set_xticks([]) # Turn off x ticks
    plt.tight_layout()
    plt.show()  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/digits_each_4.png&quot; width=&quot;700&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;preprocessing&quot;&gt;Preprocessing&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;https://scikit-learn.org/stable/modules/preprocessing.html&quot;&gt;sklearn.preprocessing&lt;/a&gt; package contains numerous functions and classes to change features into a representation that would be useful for learning algorithms. It may be good practice to initially standardize a dataset, as it is usually advantageous to apply learning algorithms to a standardized dataset. If the dataset contains outliers, scalars or transformers might be more beneficial.&lt;/p&gt;

&lt;p&gt;The goal in standardizing and normalizing is to change the values (e.g., numeric feature values within a column) in order to use a common scale, without distorting differences in the ranges of values or losing information. This can help the model overcome learning problems.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Data &lt;em&gt;normalization&lt;/em&gt; is the process of scaling individual samples to have unit norm (i.e., a vector of length 1). When a feature is normalized, the feature values are in between 0 and 1.&lt;/li&gt;
  &lt;li&gt;Data &lt;em&gt;standardization&lt;/em&gt; is the process of transforming features so that each feature looks like standard normally distributed data. And each feature is distributed about zero with a standard deviation of 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The learning algorithms might not work as well if the individual features do not appear similar to standard normally distributed data (i.e., Gaussian with zero mean and unit variance). For example, it is assumed by elements in the objective function of a learning algorithm that all features are centered around zero and have similar variances. But, if a feature has a variance that is much larger than other features, it might dominate the objective function of the learning algorithm and thus make the algorithm unable to learn from other features correctly. Here, the data is standardized in the code above.&lt;/p&gt;

&lt;h2 id=&quot;dimensionality-reduction&quot;&gt;Dimensionality reduction&lt;/h2&gt;
&lt;p&gt;Dimensionality is the number of features in your dataset. Dimensionality reduction is a process that reduces that number of features. It might be best to try to attain a compact representation of high-dimensional data without having to lose much information. There are many benefits in its applications. One advantage is that it reduces the processing time and storage space needed for the particular modelling algorithm. It may help avoid the effects of the curse of dimensionality. It can also improve the interpretation of the parameters of the machine learning model. Also, in order to visualize data, it needs to be reduced to very low dimensions (2D or 3D). There appears to be two key dimensionality reduction methods:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Feature selection – A subset is selected from the original feature set.&lt;/li&gt;
  &lt;li&gt;Feature extraction - A new (smaller) set of features is created from the original feature set. This new set of features captures the most useful information in the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A guide that explains both feature selection and feature extraction in detail can be found &lt;a href=&quot;https://elitedatascience.com/dimensionality-reduction-algorithms&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For comparison purposes, the following dimensionality reduction techniques will be used: principal component analysis, t-distributed Stochastic Neighbor Embedding, truncated singular value decomposition, and Isomap.&lt;/p&gt;

&lt;h3 id=&quot;principal-component-analysis&quot;&gt;Principal component analysis&lt;/h3&gt;
&lt;p&gt;Principal component analysis (PCA) is a technique that reduces dimensionality of a dataset that comprises several interrelated variables, while retaining much of the variation within the dataset. This occurs by transforming to a new set of variables, the principal components (PCs). These PCs are not correlated and they are ordered such that the first few of them retain most of the variation contained within all of the original variables.&lt;/p&gt;

&lt;p&gt;The idea of PCA is to express the data in terms of a few well-chosen vectors that more efficiently capture the variation in that data. Generally, the goal is to save space while avoiding redundancies and eliminating the least important information. PCA is one of the most widely used techniques for dimensionality reduction and it is often used with large datasets with many features. 
This technique finds a combination of features that capture the variance of the original features well. Thus, I thought it best to use this technique first in order to have a look at what it does to the dataset. I won’t delve into the mathematics here, but the derivation of the principal components (and much additional information) can be found &lt;a href=&quot;https://www.springer.com/gb/book/9780387954424&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, let’s visualize the original data (i.e., this data is not standardized). We’ll project the original data from high-dimensional (64 dimensions) space into lower-dimensional principal component space (2 dimensions). We’ll color the data points by their corresponding digit class, and thus see how well these classes have been separated in principal component space.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# The first two principal components are generated below (on the non-standardized data)
pca2_result = PCA(n_components=2).fit_transform(digits.data) 
plt.scatter(pca2_result[:, 0], pca2_result[:, 1],
                c=digits.target, edgecolor='none', alpha=0.5,
                cmap=plt.cm.get_cmap('tab10', 10))
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.colorbar()
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pca_digits_non_standardized.png&quot; width=&quot;700&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, the explained variation per principal component is ~12% and ~9%. Thus, the first two principal components account for ~22% of the variation in the entire dataset. See the code below:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pca = PCA(n_components=2)  
pca_result = pca.fit_transform(data)  
print ('Explained variation per principal component: 	{}'.format(pca.explained_variance_ratio_))  
print ('The first two components account for {:.0f}% of the variation in the 	entire dataset'.format((pca.explained_variance_ratio_[0] + 	pca.explained_variance_ratio_[1])*100))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And ~50% of the variance is contained within 5 principal components. One may estimate how many components are needed to describe the data by assessing the cumulative explained variance ratio as a function of the number of components. Approximately 90% of the variance is retained within 20 components, whereas 75% is retained within 10 components. See the code and corresponding plot below of the cumulative sum of the percentage of variance explained by the components.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pca4 = PCA().fit(digits.data) # Conducting PCA without specifying a number of
plt.plot(np.cumsum(pca4.explained_variance_ratio_)) # The cumulative sum of the percentage of variance explained by all of the components
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
plt.title('Number of components vs. retained variance')
plt.show() 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/n_comp_var.png&quot; width=&quot;650&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This data from scikit-learn is stored in the .data member, which is an (n_samples, n_features) array. This array changed from the initial shape of the array (1797, 64) to the current shape (1797, 2) via PCA reduction. Thus, the vector length has now been compressed to a length of 2. But has important data been lost by conducting this PCA reduction? Let’s plot the inverse transform of the PCA reduced data below the original data. The inverse transform can illustrate the reconstructed images after having undergone PCA reduction.&lt;/p&gt;

&lt;p&gt;Here is an example of the original data:
&lt;img src=&quot;/assets/img/digits0_9_original.png&quot; width=&quot;800&quot; height=&quot;180&quot; /&gt;
Here is that same example seen directly above, but this figure is the inverse transform of the data:
&lt;img src=&quot;/assets/img/digits0_9_pca.png&quot; width=&quot;800&quot; height=&quot;180&quot; /&gt;
The original 64 dimensions were reduced to 2. While PCA has reduced the dimensionality of the data by a factor of 32, the reconstructed images contain enough information that one might visually recognize the individual digits in the figure. Although it doesn’t seem that much of the variance is retained (~22%) within the first two principal components, it does appear that PCA reduction works for this dataset.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">A Brief Introduction to Object Oriented Programming</title><link href="http://localhost:4000/2019/04/25/object-oriented-programming.html" rel="alternate" type="text/html" title="A Brief Introduction to Object Oriented Programming" /><published>2019-04-25T00:00:00+01:00</published><updated>2019-04-25T00:00:00+01:00</updated><id>http://localhost:4000/2019/04/25/object-oriented-programming</id><content type="html" xml:base="http://localhost:4000/2019/04/25/object-oriented-programming.html">&lt;h2 id=&quot;what-is-object-oriented-programming&quot;&gt;What is Object Oriented Programming?&lt;/h2&gt;
&lt;p&gt;Object oriented programming (OOP) is a pattern of programming in which the solution to a programming problem is modelled as a collection of collaborating objects.&lt;/p&gt;

&lt;p&gt;Prior to the development of OOP, procedural programming divided a program into a set of functions. The data was stored in variables and the functions operated on that data. It was relatively simple, however, complications would arise when functions were changed. For example, another function may depend on the function that was changed, which could lead to the code becoming difficult to maintain. Essentially, OOP came about to help with this and other programming problems. Some benefits of using OOP include: reducing complexity, isolating the impact of changes made to code, eliminating redundant code, and refactoring code (e.g., condensing lengthy if/else statements). It also reduces development time by supporting programming by customization.&lt;/p&gt;

&lt;h4 id=&quot;why-use-oop-in-python&quot;&gt;Why use OOP in Python?&lt;/h4&gt;
&lt;p&gt;In Python, you could use OOP to build a data type(s) that matches real world objects. You can re-use and build upon code. You can work with abstract objects related to your domain.&lt;/p&gt;

&lt;p&gt;Python is an object oriented language. For example, its class model supports polymorphism, operator overloading, and multiple inheritance (see: &lt;a href=&quot;http://shop.oreilly.com/product/0636920028154.do&quot;&gt;Learning Python&lt;/a&gt; by Mark Lutz). But OOP is an option in Python, you don’t need to become an OOP expert and immediately start applying it because Python supports a procedural programming mode as well.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-four-core-principles-of-oop&quot;&gt;What are the four core principles of OOP?&lt;/h2&gt;
&lt;p&gt;It is often said that there are four main concepts in OOP, which are: encapsulation, abstraction, inheritance, and polymorphism. 
I generally try to find a mnemonic way to remember things. What came to mind when I first learned this was, apple (a from apple is the first letter in &lt;em&gt;abstraction&lt;/em&gt;) pie, or even ‘a pie.’&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A - abstraction&lt;/li&gt;
  &lt;li&gt;P - polymorphism&lt;/li&gt;
  &lt;li&gt;I  - inheritance&lt;/li&gt;
  &lt;li&gt;E – encapsulation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While it worked for me to remember these concepts, lets look at these concepts in a logical order to better understand them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Encapsulation&lt;/strong&gt; – It wraps up the data and code into a single unit (i.e., a class). This keeps the data and code safe from outside interference and misuse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstraction&lt;/strong&gt; – This can be thought of as an extension of encapsulation. Abstraction is the process of hiding unnecessary details from the user. Essentially, it hides the complexity and provides only essential information to the user.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inheritance&lt;/strong&gt; – It’s the process of creating a new object or class (called a derived class or subclass or child class) from an existing object or class (called a base class or super class or parent class), while retaining similar implementation. The derived class inherits all of the capabilities of the base class, but it can be extended and refined. Basically, the derived class is a more specialized one and it provides reusability. There are multiple types of inheritance, which include: single, multiple, multi-level, hierarchical, and hybrid. These types are based on the paradigm and specific language.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Polymorphism&lt;/strong&gt; – The word means ‘having many forms’. It refers to the ability to process objects differently depending on their data type or class (I really like this definition found here: &lt;a href=&quot;https://www.webopedia.com/TERM/P/polymorphism.html&quot;&gt;Polymorphism&lt;/a&gt;). One is able to redefine methods for derived classes. It essentially describes the concept that objects of different types can be accessed through the same interface. And each type can provide its own, independent implementation of this interface.&lt;/p&gt;

&lt;p&gt;These OOP concepts can help our work become organized, modular and maintainable. They help us to omit repetition, keep data safe, and make our work user friendly.&lt;/p&gt;

&lt;h2 id=&quot;what-are-the-solid-principles&quot;&gt;What are the SOLID Principles?&lt;/h2&gt;
&lt;p&gt;SOLID is an acronym for five class design principles that are intended to help a software design become more understandable, flexible, and easier to maintain. SOLID can be used as a checklist, something worth revisiting to help you create a well-made class design, but each part may not be necessary to implement.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/solid.png&quot; width=&quot;500&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Robert C. Martin (known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Robert_C._Martin&quot;&gt;“Uncle Bob”&lt;/a&gt;) defined principles of OOD, which can be found here: &lt;a href=&quot;http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod&quot;&gt;SOLID principles&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;single-responsibility-principle-srp&quot;&gt;&lt;strong&gt;Single responsibility principle&lt;/strong&gt; (&lt;a href=&quot;https://drive.google.com/file/d/0ByOwmqah_nuGNHEtcU5OekdDMkk/view&quot;&gt;SRP&lt;/a&gt;)&lt;/h4&gt;
&lt;p&gt;“A class should have one, and only one, reason to change.”&lt;/p&gt;

&lt;p&gt;An object/class should have one primary responsibility. The object should have one reason to exist and that reason should be encapsulated within the class. It can have different behaviors, but those behaviors are oriented around that sole reason for existence. One may not want to create god objects that try to do too many different things.&lt;/p&gt;

&lt;h4 id=&quot;openclosed-principle-ocp&quot;&gt;&lt;strong&gt;Open/closed principle&lt;/strong&gt; (&lt;a href=&quot;https://drive.google.com/file/d/0BwhCYaYDn8EgN2M5MTkwM2EtNWFkZC00ZTI3LWFjZTUtNTFhZGZiYmUzODc1/view&quot;&gt;OCP&lt;/a&gt;)&lt;/h4&gt;
&lt;p&gt;“You should be able to extend a classes behavior, without modifying it.”&lt;/p&gt;

&lt;p&gt;A system should be open for extension, but closed for modification. For example, if you want to add a feature, you could do this by writing new code, without modifying the existing code. Robert Martin provides an updated definition of the initial one provided by (Meyer 1988).&lt;/p&gt;

&lt;h4 id=&quot;liskov-substitution-principle-lsp&quot;&gt;&lt;strong&gt;Liskov substitution principle&lt;/strong&gt; (&lt;a href=&quot;https://drive.google.com/file/d/0BwhCYaYDn8EgNzAzZjA5ZmItNjU3NS00MzQ5LTkwYjMtMDJhNDU5ZTM0MTlh/view&quot;&gt;LSP&lt;/a&gt;)&lt;/h4&gt;
&lt;p&gt;“Derived classes must be substitutable for their base classes.”&lt;/p&gt;

&lt;p&gt;Objects in a program should be replaceable with instances of their subclasses (or child classes or derived classes, whichever term you use for them) without altering the correctness of the program. So, you should be able to treat the child classes like their parent classes. As Robert Martin states, “Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.”&lt;/p&gt;

&lt;h4 id=&quot;interface-segregation-principle-isp&quot;&gt;&lt;strong&gt;Interface segregation principle&lt;/strong&gt; (&lt;a href=&quot;https://drive.google.com/file/d/0BwhCYaYDn8EgOTViYjJhYzMtMzYxMC00MzFjLWJjMzYtOGJiMDc5N2JkYmJi/view&quot;&gt;ISP&lt;/a&gt;)&lt;/h4&gt;
&lt;p&gt;“Make fine grained interfaces that are client specific.”&lt;/p&gt;

&lt;p&gt;Many specific interfaces are better than one general purpose interface. The idea of the interface here is having formalized lists of method names that a class can choose to support. And those lists of methods should be as small as possible and if they start to get bigger, then they should be split up into smaller interfaces as classes can choose to implement multiple smaller interfaces. A class should not have to support enormous lists of methods that it doesn’t need.&lt;/p&gt;

&lt;h4 id=&quot;dependency-inversion-principle-dip&quot;&gt;&lt;strong&gt;Dependency Inversion Principle&lt;/strong&gt; (&lt;a href=&quot;https://drive.google.com/file/d/0BwhCYaYDn8EgMjdlMWIzNGUtZTQ0NC00ZjQ5LTkwYzQtZjRhMDRlNTQ3ZGMz/view&quot;&gt;DIP&lt;/a&gt;)&lt;/h4&gt;
&lt;p&gt;“Depend on abstractions, not on concretions.”&lt;/p&gt;

&lt;p&gt;Depend on abstractions, not on concretions. This is the idea of not directly tying concrete objects together, but making them deal with abstractions in order to minimize dependencies between our objects. For example, if you have a high-level object, which depends on two low-level objects - you disconnect these two very concrete classes (i.e., remove the two low-level objects that are very specific), so you remove those connections and then insert a new layer between them, which might be abstract classes that are more basic. Therefore, it allows substitutions and flexibility. You can replace and extend the object.&lt;/p&gt;

&lt;p&gt;Using some or all of the above principles can help keep your code maintainable, testable, and it can reduce the complexity. That’s a win-win result in my book!&lt;/p&gt;</content><author><name></name></author><summary type="html">What is Object Oriented Programming? Object oriented programming (OOP) is a pattern of programming in which the solution to a programming problem is modelled as a collection of collaborating objects.</summary></entry></feed>